---
title: "Rough Draft Literature Review - Data Science Capstone"
author: "Lindsay Guyette, Alondra Nunez, Chantal Ojurongbe, Jonathan Broada"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

The objectives of this review are threefold: to provide a comprehensive overview of K-means clustering, to discuss its methodology, applications, and advancements, and to identify the challenges and future research directions. In this literature review, we aim to give an in-depth analysis of K-means clustering, examine its various aspects and practical uses, and highlight the current challenges along with potential areas for future research.

Clustering is the process of grouping similar objects or data points together based on their characteristics or attributes. Clustering is an unsupervised learning technique employed in data mining and machine learning to categorize a collection of items based on their similarity. The goal is to create groupings, or clusters, where objects within the same cluster are more alike to each other than to objects in other clusters. Clustering aims to identify inherent clusters in a dataset where the data points within each cluster exhibit a strong resemblance, yet the clusters themselves are clearly dissimilar from one another.

The similarity of data points is commonly assessed using a distance metric, such as Euclidean distance, Manhattan distance, or cosine similarity. The choice of metric depends on the characteristics of the data and the specific clustering technique employed [@Berkhin2006].

The term "k-Means Clustering" refers to a method used in data analysis and machine learning to partition a set of data points into k distinct clusters. k-Means clustering is a highly popular and extensively utilized approach for clustering. The objective is to divide a dataset into k clusters, where k is a predetermined number of clusters. The method operates in an iterative manner, assigning each data point to one of k clusters based on the given attributes. The main concept is to establish k centroids, representing each cluster, and allocate each data point to the centroid that is closest to it. This allocation is done in a way that minimizes the total squared distances between the data points and their respective centroids.

The k-means clustering algorithm can be analytically defined as an optimization problem. The objective is to divide a collection of n data points into k clusters in a manner that minimizes the sum of squares inside each cluster (WCSS). The process entails the following steps: Objective function (minimization of within-cluster variance). The goal of k-means is to minimize the total sum of squared distances between each data point and its corresponding cluster centroid. The **within-cluster sum of squares (WCSS)** represents this.

$$\text{WCSS} = \sum_{j=1}^{k}\sum_{xi\in Cj}^{} \left\| x_{i} - \mu_{j} \right\|^2$$

where: C~j~ is the set of data points assigned to cluster j, x~i~ is a data point, μ~j​~ is the centroid of cluster j, ∥x~i~−μ~j~∥ is the Euclidean distance between data point x~i~​ and centroid μ~j~​. Mathematical representation of the algorithm. Algorithm Steps in Mathematical Terms Initialization Randomly choose k initial centroids μ~1~,μ~2~,...,μ~k​~ from the dataset {x~1~,x~2~,...,x~n~}. Assignment Step Assign each data point x~i​~ to the nearest centroid μ~j~​. This can be mathematically expressed as: C~j~ = {x~i~ : ∥x~i~−μ~j~∥2 ≤ ∥x~i~−μ~l~∥2 for all l, 1 ≤ l ≤ k} Here, C~j​~ denotes the set of points assigned to centroid μ~j~​ [@hastie_elements_2009].

K-means clustering can be applied to a wide variety of data, including bioinformatics, sociology, and in food markets. **Bioinformatics** refers to the usage of analytical techniques to interpret different types of biological data. These datasets can be large or small, containing information about genomic sequences, gene expression, and protein expression, among other biological characteristics. This information can then be used to draw conclusions about basic biological processes, disease states, and gene therapies, among other applications [@bayat_bioinformatics_2002]. K-means clustering is one type of data analysis technique that can be used to group differential gene/protein expression according to specific phenotypes, or physical presentations of biological traits. For instance, this can be used to assign cells to specific types based on single-cell RNA sequencing data, which measures the total gene expression for thousands of genes within a single cell. Yang et al., 2017 utilized k-means clustering to analyze the scRNA sequencing data of hundreds of cell types to develop a novel algorithm that identifies the optimal sets of genes to cluster single cells into distinct biological groups.

Additionally, k-means clustering can be applied to population-level data to determine significant groupings related to **socioeconomic outcomes.** The factors related to socioeconomic mobility were evaluated in a 2023 study using longitudinal data from the Opportunity Atlas project. Here, k-means clustering revealed differences in socioeconomic mobility related to geographic location, income, and neighborhood factors such as poverty rate and incarceration rates [@zelasky_identifying_2023].

In 2017, food market researchers used k-means clustering to segment the organic food market in Lebanon. By starting with 13 variables, they applied Principal Component Analysis (PCA) to reduce the number while retaining the explanatory power. The k-means algorithm was then employed, resulting in four distinct clusters of consumer opinions on organic food. Each cluster was labeled appropriately to facilitate understanding and application in research. The outcomes present relevant groups that can guide policy and initiatives in the organic food market in Lebanon [@tleis_segmenting_2017].

Variations and enhancements of the k-means clustering algorithm have been thoroughly researched. The k-means algorithm was originally proposed in in the 1950s and due to its computational simplicity, it has many challenges (like having to select the number of clusters, or minimal local convergence because of the random initial centroids) many variants of k-means have been created to tackle those challenges and broaden the applicability of k-means [@ikotun_k-means_2023].

One such issue is the selection of the number of clusters in k-means clustering. In a 2013 review, researcher Kodinariya explains how to estimate the number of clusters. Fields across the board use clustering so there is no one answer on how many clusters to use. Kodinariya explores six different ways on how to determine the best number of clusters. The six methods are: rule of thumb, elbow method, information criterion approach, information theoretic approach, Silhouette, and cross validation. Kodinariya explains how no size fits all and will depend on the context of the data. The elbow method is the oldest and most used method but information theoretic approach is more rigorous. It is best to use more than one method to estimate the best number of clusters [@kodinariya_review_2013].

Yang et al propose a new method on selecting the optimal k value by addressing both application-specific and detector-specific factors. The authors proposed the KFC method which offers a parameter free solution with linear time complexity. Yang et al made the KFC method available at www.outliernet.com to an external site to facilitate reproducibility and provide opportunities for others to expand their research. Their research showed that the KFC method outperforms traditional methods and is much more versatile making it useful across many fields [@yang_outlier_2023].

The article "K-means clustering with outlier removal" by Guojun Gan and Michael Kwok-Po Ng proposes to improve the k-means clustering algorithm by including outlier detection. The KMOR (k-means with outlier removal) algorithm broadens the classic k-means algorithm by proposing an additional cluster for outliers. The methodology comprises initialization, objective function optimization, iterative process updates, convergence, and parameter control. The KMOR algorithm was tested on synthetic and real datasets. It showed remarkable performance in overall clustering accuracy and outlier detection. Moreover, compared to other established algorithms such as ODC (Outlier Detection and clustering), k-means, and NEO-k-means algorithms, results have confirmed that KMOR algorithms not only outperformed them but also consistently cluster data and detect outliers with precision simultaneously. The KMOR algorithm's ability to seamlessly integrate clustering and outlier detection for both synthetic and real datasets showcased its dominance over current methods in terms of accuracy and efficiency. Futuristically, the goals are to investigate diverse ways to control outliers by refining parameter selection and broadening the algorithm for subspace clustering [@gan_k-means_2017].

## Methodology

After importing the dataset, the expression for 77 proteins data are scaled and analyzed using Principle Component Analysis to reduce the number of dimensions. Then, the optimal number of clusters is determined using the elbow method, silhouette analysis, and gap statistic.The data are then analyzed using the k-means algorithm with the default euclidean distance metric. Differences are then considered using a contingency table and heat map to identify similar and dissimlar groups. These cluster assignments are then plotted against class to visually identify cluster characteristics.

## Data Analysis

The data are sourced from a 2015 paper which evaluates the effect on protein expression of the Alzheimer's drug memantine in rescuing learning in Ts65Dn trisomic mice relative to control mice. Ts65Dn mice are genetically modified to be trisomic for 2/3 of the genes orthologous to human chromosome 21 (Hsa21), making them a suitable model organism to experimentally study Down's Syndrome (DS) (jax citation). It is suspected that memantine can be used to treat learning deficits in DS, and this rescue effect has been previously observed in the Ts65Dn model. This study evaluates the effect of these different factors on the expression of 77 different proteins involved in Alzheimer's, neuronal receptors, Hsa21 genes, and homeostatic genes. Therefore, it is expected that Ts65Dn mice treated with memantine in the learning condition will exhibit similar patterns of protein expression relative to the untreated control model in the learning condition. Protein expression data provided are normalized to total protein expression [@higuera_self-organizing_2015].

### Load Necessary Libraries

```{r}
#install.packages("NbClust")
#install.packages("fpc")
library(fpc)
library(NbClust)
library(cluster)

```

```{r}
#install.packages("tidyverse")
#install.packages("factoextra")
#install.packages("cluster")
#install.packages("pheatmap")

library(tidyverse)
library(factoextra)
library(cluster)
library(pheatmap)
```

1.  **Load the data**:

    ```{r}
    data <- read.csv("mice_new_jonathan.csv")
    ```

```{r}
# Selecting protein expression columns (2 to 78) and scaling the data
protein_data <- data[, 2:78]
protein_data_scaled <- scale(protein_data)
```

### Scale Data and PCA

```{r}
protein_data_scaled <- scale(protein_data)
pca_result <- prcomp(protein_data_scaled, center = TRUE, scale. = TRUE)
summary(pca_result)
fviz_eig(pca_result)
pca_data <- pca_result$x[, 1:2] 
```

### Explore Optimal Number of Clusters

```{r}
# Elbow Method
fviz_nbclust(pca_data, kmeans, method = "wss")

# Silhouette Analysis
fviz_nbclust(pca_data, kmeans, method = "silhouette")

# Gap Statistic

gap_stat <- clusGap(pca_data, FUN = kmeans, nstart = 25, K.max = 20, B = 50)
fviz_gap_stat(gap_stat)
```

### Perform K-means with Optimal Number of Clusters

```{r}
# Set optimal number of clusters, assuming we found 8 as optimal from previous step
optimal_clusters <- 3

kmeans_result <- kmeans(pca_data, centers = optimal_clusters, nstart = 25)

# Visualize Clusters
fviz_cluster(kmeans_result, data = pca_data, geom =c("point"))
```

### Evaluate Cluster Stability

```{r}

# Add the cluster assignments to the original data
data$cluster <- as.factor(kmeans_result$cluster)

# Create a table to compare original classes and clusters
comparison_table <- table(data$class_short, data$cluster)
print(comparison_table)

# Create a contingency table 
contingency_table <- table(data$class_short, data$cluster)


# Plot the heatmap
pheatmap(contingency_table, 
         cluster_rows = FALSE, 
         cluster_cols = FALSE, 
         display_numbers = TRUE,
         color = colorRampPalette(c("white", "blue"))(50),
         main = "Heatmap of Original Classes vs Clusters")
```

```{r}
color_palette <- c(
  "c-CS-s" = "#4f9ed4",
  "c-CS-m" = "#80bbdf",
  "c-SC-s" = "#6fbf67",
  "c-SC-m" = "#96d991",
  "t-CS-s" = "#194e85",
  "t-CS-m" = "#133a62",
  "t-SC-s" = "#256a27",
  "t-SC-m" = "#1a501d"
)

# Plot cluster assignments against the original classes
ggplot(data, aes(x = cluster, fill = class_short)) + 
  geom_bar(position = "stack") +
  scale_fill_manual(values = color_palette) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Cluster Assignments by Class",
       x = "Cluster",
       y = "Count",
       fill = "Class")
```

### Plot Clusters with Color-Coding by Behavior

```{r}

combined_data <- cbind(data, pca_data)

# Add cluster assignment to the combined data
combined_data$cluster <- factor(kmeans_result$cluster)

# Create convex hull plots for each cluster
hulls <- combined_data %>%
  group_by(cluster) %>%
  slice(chull(PC1, PC2))

# Plot PCA data with clusters colored by Behavior and shaded by clusters
ggplot(combined_data, aes(x = PC1, y = PC2, color = Behavior)) +
  geom_polygon(data = hulls, aes(x = PC1, y = PC2, fill = cluster), alpha = 0.2) +
  geom_point(aes(shape = cluster), size = 2) +
  labs(title = "PCA Plot with K-means Clusters (Shaded by Cluster and Color-Coded by Behavior)",
       x = "Principal Component 1",
       y = "Principal Component 2") +
  scale_color_manual(values = c("CS" = "blue", "SC" = "red")) +  # Adjust colors according to your data
  scale_fill_manual(values = rainbow(optimal_clusters)) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

## Conclusions

Our results indicate that the mice protein expression data set exhibits clusters which largely correspond to the learning and non-learning behavior groups.No such clustering is observed with regard to genotype or treatment group based on the considerations of class composition by cluster.
