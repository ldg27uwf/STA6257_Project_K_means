
@article{zelasky_identifying_2023,
	title = {Identifying groups of children's social mobility opportunity for public health applications using k-means clustering},
	volume = {9},
	issn = {2405-8440},
	url = {https://www.sciencedirect.com/science/article/pii/S2405844023074583},
	doi = {10.1016/j.heliyon.2023.e20250},
	abstract = {Background
The Opportunity Atlas project is a pioneering effort to trace social mobility and adulthood socioeconomic outcomes back to childhood residence. Half of the variation in adulthood socioeconomic outcomes was explainable by neighborhood-level socioeconomic characteristics during childhood. Clustering census tracts by Opportunity Atlas characteristics would allow for further exploration of variance in social mobility. Our objectives here are to identify and describe spatial clustering trends within Opportunity Atlas outcomes.
Methods
We utilized a k-means clustering machine learning approach with four outcome variables (individual income, incarceration rate, employment, and percent of residents living in a neighborhood with low levels of poverty) each given at five parental income levels (1st, 25th, 50th, 75th, and 100th percentiles of the national distribution) to create clusters of census tracts across the contiguous United States ({US}) and within each Environmental Protection Agency region.
Results
At the national level, the algorithm identified seven distinct clusters; the highest opportunity clusters occurred in the Northern Midwest and Northeast, and the lowest opportunity clusters occurred in rural areas of the Southwest and Southeast. For regional analyses, we identified between five to nine clusters within each region. {PCA} loadings fluctuate across parental income levels; income and low poverty neighborhood residence explain a substantial amount of variance across all variables, but there are differences in contributions across parental income levels for many components.
Conclusions
Using data from the Opportunity Atlas, we have taken four social mobility opportunity outcome variables each stratified at five parental income levels and created nationwide and {EPA} region-specific clusters that group together census tracts with similar opportunity profiles. The development of clusters that can serve as a combined index of social mobility opportunity is an important contribution of this work, and this in turn can be employed in future investigations of factors associated with children's social mobility.},
	pages = {e20250},
	number = {9},
	journaltitle = {Heliyon},
	shortjournal = {Heliyon},
	author = {Zelasky, Sarah and Martin, Chantel L. and Weaver, Christopher and Baxter, Lisa K. and Rappazzo, Kristen M.},
	urldate = {2024-07-07},
	date = {2023-09-01},
	keywords = {Clustering, K-means, Opportunity, Socioeconomics, Upward social mobility},
	file = {Full Text:C\:\\Users\\linds\\Zotero\\storage\\95TGMKR3\\Zelasky et al. - 2023 - Identifying groups of children's social mobility o.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\linds\\Zotero\\storage\\TF3V4SJT\\S2405844023074583.html:text/html},
}

@article{sadeghi_principal_2024,
	title = {Principal components analysis and K-means clustering of till geochemical data: Mapping and targeting of prospective areas for lithium exploration in Västernorrland Region, Sweden},
	volume = {167},
	issn = {0169-1368},
	url = {https://www.sciencedirect.com/science/article/pii/S0169136824001355},
	doi = {10.1016/j.oregeorev.2024.106002},
	shorttitle = {Principal components analysis and K-means clustering of till geochemical data},
	abstract = {To achieve the demand for elements used for the green transition energy, such as lithium, it is necessary to recognize the spatial distribution of the concentrations of these elements in different earth materials such as bedrock and soil and to identify areas with anomalous concentrations of such elements (i.e., mineralization) for further exploration and hopefully exploitation. This study carried out multivariate statistical analyses on compositional (i.e., element concentration) data from till samples to recognize areas that likely contain lithium pegmatite mineralization in the Västernorrland region, central Sweden. We applied principal components analysis ({PCA}) and K-means clustering techniques to reveal regional-scale patterns in the till geochemical data. We demonstrate that these two methods have potential for recognition of geochemical anomalies related to the underlying bedrock geology as well as to mineralization. The results of {PCA}- and K-means clustering were validated using known occurrences of lithium mineralization. Two different datasets were compared; one containing all available geochemical data and the second containing only available trace elements in the dataset and it was found that anomalous clusters of samples defined by K-means clustering have anomalous multi-element signatures defined by robust {PCA}. This demonstrated that principal components are the continuous solutions to the discrete cluster members for the K-means clustering. The results show that both {PCA} and K-means clustering of till geochemical datasets at the early stages of exploration and target generation may reveal useful information that can be used to identify potential areas for more detailed mapping or exploration activities.},
	pages = {106002},
	journaltitle = {Ore Geology Reviews},
	shortjournal = {Ore Geology Reviews},
	author = {Sadeghi, Martiya and Casey, Patrick and Carranza, Emmanuel John M. and Lynch, Edward P.},
	urldate = {2024-07-07},
	date = {2024-04-01},
	keywords = {K-means clustering, Lithium pegmatite mineralization, Principal component analysis, Target generation},
	file = {ScienceDirect Snapshot:C\:\\Users\\linds\\Zotero\\storage\\B7MLM5PJ\\S0169136824001355.html:text/html},
}

@article{richards_comparison_2008,
	title = {A comparison of four clustering methods for brain expression microarray data},
	volume = {9},
	issn = {1471-2105},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2655095/},
	doi = {10.1186/1471-2105-9-490},
	abstract = {Background
{DNA} microarrays, which determine the expression levels of tens of thousands of genes from a sample, are an important research tool. However, the volume of data they produce can be an obstacle to interpretation of the results. Clustering the genes on the basis of similarity of their expression profiles can simplify the data, and potentially provides an important source of biological inference, but these methods have not been tested systematically on datasets from complex human tissues. In this paper, four clustering methods, {CRC}, k-means, {ISA} and {memISA}, are used upon three brain expression datasets. The results are compared on speed, gene coverage and {GO} enrichment. The effects of combining the clusters produced by each method are also assessed.

Results
k-means outperforms the other methods, with 100\% gene coverage and {GO} enrichments only slightly exceeded by {memISA} and {ISA}. Those two methods produce greater {GO} enrichments on the datasets used, but at the cost of much lower gene coverage, fewer clusters produced, and speed. The clusters they find are largely different to those produced by k-means. Combining clusters produced by k-means and {memISA} or {ISA} leads to increased {GO} enrichment and number of clusters produced (compared to k-means alone), without negatively impacting gene coverage. {memISA} can also find potentially disease-related clusters. In two independent dorsolateral prefrontal cortex datasets, it finds three overlapping clusters that are either enriched for genes associated with schizophrenia, genes differentially expressed in schizophrenia, or both. Two of these clusters are enriched for genes of the {MAP} kinase pathway, suggesting a possible role for this pathway in the aetiology of schizophrenia.

Conclusion
Considered alone, k-means clustering is the most effective of the four methods on typical microarray brain expression datasets. However, {memISA} and {ISA} can add extra high-quality clusters to the set produced by k-means, so combining these three methods is the method of choice.},
	pages = {490},
	journaltitle = {{BMC} Bioinformatics},
	shortjournal = {{BMC} Bioinformatics},
	author = {Richards, Alexander L and Holmans, Peter and O'Donovan, Michael C and Owen, Michael J and Jones, Lesley},
	urldate = {2024-07-07},
	date = {2008-11-25},
	pmid = {19032745},
	pmcid = {PMC2655095},
	file = {PubMed Central Full Text PDF:C\:\\Users\\linds\\Zotero\\storage\\FEVVLJ8W\\Richards et al. - 2008 - A comparison of four clustering methods for brain .pdf:application/pdf},
}

@article{yang_saic_2017,
	title = {{SAIC}: an iterative clustering approach for analysis of single cell {RNA}-seq data},
	volume = {18},
	issn = {1471-2164},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5629617/},
	doi = {10.1186/s12864-017-4019-5},
	shorttitle = {{SAIC}},
	abstract = {Background
Research interests toward single cell analysis have greatly increased in basic, translational and clinical research areas recently, as advances in whole-transcriptome amplification technique allow scientists to get accurate sequencing result at single cell level. An important step in the single-cell transcriptome analysis is to identify distinct cell groups that have different gene expression patterns. Currently there are limited bioinformatics approaches available for single-cell {RNA}-seq analysis. Many studies rely on principal component analysis ({PCA}) with arbitrary parameters to identify the genes that will be used to cluster the single cells.

Results
We have developed a novel algorithm, called {SAIC} (Single cell Analysis via Iterative Clustering), that identifies the optimal set of signature genes to separate single cells into distinct groups. Our method utilizes an iterative clustering approach to perform an exhaustive search for the best parameters within the search space, which is defined by a number of initial centers and P values. The end point is identification of a signature gene set that gives the best separation of the cell clusters. Using a simulated data set, we showed that {SAIC} can successfully identify the pre-defined signature gene sets that can correctly separated the cells into predefined clusters. We applied {SAIC} to two published single cell {RNA}-seq datasets. For both datasets, {SAIC} was able to identify a subset of signature genes that can cluster the single cells into groups that are consistent with the published results. The signature genes identified by {SAIC} resulted in better clusters of cells based on {DB} index score, and many genes also showed tissue specific expression.

Conclusions
In summary, we have developed an efficient algorithm to identify the optimal subset of genes that separate single cells into distinct clusters based on their expression patterns. We have shown that it performs better than {PCA} method using published single cell {RNA}-seq datasets.

Electronic supplementary material
The online version of this article (doi:10.1186/s12864-017-4019-5) contains supplementary material, which is available to authorized users.},
	pages = {689},
	issue = {Suppl 6},
	journaltitle = {{BMC} Genomics},
	shortjournal = {{BMC} Genomics},
	author = {Yang, Lu and Liu, Jiancheng and Lu, Qiang and Riggs, Arthur D. and Wu, Xiwei},
	urldate = {2024-07-07},
	date = {2017-10-03},
	pmid = {28984204},
	pmcid = {PMC5629617},
	file = {PubMed Central Full Text PDF:C\:\\Users\\linds\\Zotero\\storage\\JNFI5RYF\\Yang et al. - 2017 - SAIC an iterative clustering approach for analysi.pdf:application/pdf},
}

@article{bayat_bioinformatics_2002,
	title = {Bioinformatics},
	volume = {324},
	issn = {0959-8138},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1122955/},
	pages = {1018--1022},
	number = {7344},
	journaltitle = {{BMJ} : British Medical Journal},
	shortjournal = {{BMJ}},
	author = {Bayat, Ardeshir},
	urldate = {2024-07-07},
	date = {2002-04-27},
	pmid = {11976246},
	pmcid = {PMC1122955},
	file = {PubMed Central Full Text PDF:C\:\\Users\\linds\\Zotero\\storage\\JTMY4TR9\\Bayat - 2002 - Bioinformatics.pdf:application/pdf},
}

@article{yang_saic_2017-1,
	title = {{SAIC}: an iterative clustering approach for analysis of single cell {RNA}-seq data},
	volume = {18},
	issn = {1471-2164},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5629617/},
	doi = {10.1186/s12864-017-4019-5},
	shorttitle = {{SAIC}},
	abstract = {Background
Research interests toward single cell analysis have greatly increased in basic, translational and clinical research areas recently, as advances in whole-transcriptome amplification technique allow scientists to get accurate sequencing result at single cell level. An important step in the single-cell transcriptome analysis is to identify distinct cell groups that have different gene expression patterns. Currently there are limited bioinformatics approaches available for single-cell {RNA}-seq analysis. Many studies rely on principal component analysis ({PCA}) with arbitrary parameters to identify the genes that will be used to cluster the single cells.

Results
We have developed a novel algorithm, called {SAIC} (Single cell Analysis via Iterative Clustering), that identifies the optimal set of signature genes to separate single cells into distinct groups. Our method utilizes an iterative clustering approach to perform an exhaustive search for the best parameters within the search space, which is defined by a number of initial centers and P values. The end point is identification of a signature gene set that gives the best separation of the cell clusters. Using a simulated data set, we showed that {SAIC} can successfully identify the pre-defined signature gene sets that can correctly separated the cells into predefined clusters. We applied {SAIC} to two published single cell {RNA}-seq datasets. For both datasets, {SAIC} was able to identify a subset of signature genes that can cluster the single cells into groups that are consistent with the published results. The signature genes identified by {SAIC} resulted in better clusters of cells based on {DB} index score, and many genes also showed tissue specific expression.

Conclusions
In summary, we have developed an efficient algorithm to identify the optimal subset of genes that separate single cells into distinct clusters based on their expression patterns. We have shown that it performs better than {PCA} method using published single cell {RNA}-seq datasets.

Electronic supplementary material
The online version of this article (doi:10.1186/s12864-017-4019-5) contains supplementary material, which is available to authorized users.},
	pages = {689},
	issue = {Suppl 6},
	journaltitle = {{BMC} Genomics},
	shortjournal = {{BMC} Genomics},
	author = {Yang, Lu and Liu, Jiancheng and Lu, Qiang and Riggs, Arthur D. and Wu, Xiwei},
	urldate = {2024-07-07},
	date = {2017-10-03},
	pmid = {28984204},
	pmcid = {PMC5629617},
	file = {PubMed Central Full Text PDF:C\:\\Users\\linds\\Zotero\\storage\\7CHC9WCN\\Yang et al. - 2017 - SAIC an iterative clustering approach for analysi.pdf:application/pdf},
}

@article{zelasky_identifying_2023-1,
	title = {Identifying groups of children's social mobility opportunity for public health applications using k-means clustering},
	volume = {9},
	issn = {2405-8440},
	url = {https://www.sciencedirect.com/science/article/pii/S2405844023074583},
	doi = {10.1016/j.heliyon.2023.e20250},
	abstract = {Background
The Opportunity Atlas project is a pioneering effort to trace social mobility and adulthood socioeconomic outcomes back to childhood residence. Half of the variation in adulthood socioeconomic outcomes was explainable by neighborhood-level socioeconomic characteristics during childhood. Clustering census tracts by Opportunity Atlas characteristics would allow for further exploration of variance in social mobility. Our objectives here are to identify and describe spatial clustering trends within Opportunity Atlas outcomes.
Methods
We utilized a k-means clustering machine learning approach with four outcome variables (individual income, incarceration rate, employment, and percent of residents living in a neighborhood with low levels of poverty) each given at five parental income levels (1st, 25th, 50th, 75th, and 100th percentiles of the national distribution) to create clusters of census tracts across the contiguous United States ({US}) and within each Environmental Protection Agency region.
Results
At the national level, the algorithm identified seven distinct clusters; the highest opportunity clusters occurred in the Northern Midwest and Northeast, and the lowest opportunity clusters occurred in rural areas of the Southwest and Southeast. For regional analyses, we identified between five to nine clusters within each region. {PCA} loadings fluctuate across parental income levels; income and low poverty neighborhood residence explain a substantial amount of variance across all variables, but there are differences in contributions across parental income levels for many components.
Conclusions
Using data from the Opportunity Atlas, we have taken four social mobility opportunity outcome variables each stratified at five parental income levels and created nationwide and {EPA} region-specific clusters that group together census tracts with similar opportunity profiles. The development of clusters that can serve as a combined index of social mobility opportunity is an important contribution of this work, and this in turn can be employed in future investigations of factors associated with children's social mobility.},
	pages = {e20250},
	number = {9},
	journaltitle = {Heliyon},
	shortjournal = {Heliyon},
	author = {Zelasky, Sarah and Martin, Chantel L. and Weaver, Christopher and Baxter, Lisa K. and Rappazzo, Kristen M.},
	urldate = {2024-07-07},
	date = {2023-09-01},
	keywords = {Clustering, K-means, Opportunity, Socioeconomics, Upward social mobility},
	file = {Full Text:C\:\\Users\\linds\\Zotero\\storage\\MPYANDHH\\Zelasky et al. - 2023 - Identifying groups of children's social mobility o.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\linds\\Zotero\\storage\\DE3R4J5S\\S2405844023074583.html:text/html},
}

@article{lloyd_least_1982,
	title = {Least squares quantization in {PCM}},
	volume = {28},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1056489/},
	doi = {10.1109/TIT.1982.1056489},
	pages = {129--137},
	number = {2},
	journaltitle = {{IEEE} Transactions on Information Theory},
	shortjournal = {{IEEE} Trans. Inform. Theory},
	author = {Lloyd, S.},
	urldate = {2024-07-07},
	date = {1982-03},
	langid = {english},
	file = {Submitted Version:C\:\\Users\\linds\\Zotero\\storage\\YEYX826T\\Lloyd - 1982 - Least squares quantization in PCM.pdf:application/pdf},
}

@article{pan_predicting_2017,
	title = {Predicting of Power Quality Steady State Index Based on Chaotic Theory Using Least Squares Support Vector Machine},
	volume = {09},
	rights = {http://creativecommons.org/licenses/by/4.0/},
	issn = {1949-243X, 1947-3818},
	url = {http://www.scirp.org/journal/PaperDownload.aspx?DOI=10.4236/epe.2017.94B077},
	doi = {10.4236/epe.2017.94B077},
	pages = {713--724},
	number = {4},
	journaltitle = {Energy and Power Engineering},
	shortjournal = {{EPE}},
	author = {Pan, Aiqiang and Zhou, Jian and Zhang, Peng and Lin, Shunfu and Tang, Jikai},
	urldate = {2024-07-07},
	date = {2017},
	file = {Full Text:C\:\\Users\\linds\\Zotero\\storage\\2JN6PJ27\\Pan et al. - 2017 - Predicting of Power Quality Steady State Index Bas.pdf:application/pdf},
}

@article{huang_remote_2017,
	title = {Remote Sensing Image Fusion Using Bidimensional Empirical Mode Decomposition and the Least Squares Theory},
	volume = {05},
	rights = {http://creativecommons.org/licenses/by-nc/4.0/},
	issn = {2327-5219, 2327-5227},
	url = {http://www.scirp.org/journal/doi.aspx?DOI=10.4236/jcc.2017.512004},
	doi = {10.4236/jcc.2017.512004},
	pages = {35--48},
	number = {12},
	journaltitle = {Journal of Computer and Communications},
	shortjournal = {{JCC}},
	author = {Huang, Dengshan and Yang, Peng and Li, Jun and Ma, Changhui},
	urldate = {2024-07-07},
	date = {2017},
	file = {Full Text:C\:\\Users\\linds\\Zotero\\storage\\SRGH2QSV\\Huang et al. - 2017 - Remote Sensing Image Fusion Using Bidimensional Em.pdf:application/pdf},
}

@article{menke_tuning_2021,
	title = {Tuning of Prior Covariance in Generalized Least Squares},
	volume = {12},
	issn = {2152-7385, 2152-7393},
	url = {https://www.scirp.org/journal/doi.aspx?doi=10.4236/am.2021.123011},
	doi = {10.4236/am.2021.123011},
	pages = {157--170},
	number = {3},
	journaltitle = {Applied Mathematics},
	shortjournal = {{AM}},
	author = {Menke, William},
	urldate = {2024-07-07},
	date = {2021},
	file = {Full Text:C\:\\Users\\linds\\Zotero\\storage\\KK6IPVKT\\Menke - 2021 - Tuning of Prior Covariance in Generalized Least Sq.pdf:application/pdf},
}

@article{yang_quadratic_2008,
	title = {A Quadratic Constraint Total Least-squares Algorithm for Hyperbolic Location},
	volume = {01},
	rights = {http://creativecommons.org/licenses/by/4.0/},
	issn = {1913-3715, 1913-3723},
	url = {http://www.scirp.org/journal/doi.aspx?DOI=10.4236/ijcns.2008.12017},
	doi = {10.4236/ijcns.2008.12017},
	pages = {130--135},
	number = {2},
	journaltitle = {International Journal of Communications, Network and System Sciences},
	shortjournal = {{IJCNS}},
	author = {Yang, Kai and An, Jianping and Xu, Zhan},
	urldate = {2024-07-07},
	date = {2008},
	file = {Full Text:C\:\\Users\\linds\\Zotero\\storage\\74GAZXGS\\Yang et al. - 2008 - A Quadratic Constraint Total Least-squares Algorit.pdf:application/pdf},
}

@article{teng_using_2010,
	title = {Using Least Squares Support Vector Machines for Frequency Estimation},
	volume = {03},
	rights = {http://creativecommons.org/licenses/by/4.0/},
	issn = {1913-3715, 1913-3723},
	url = {http://www.scirp.org/journal/doi.aspx?DOI=10.4236/ijcns.2010.310111},
	doi = {10.4236/ijcns.2010.310111},
	pages = {821--825},
	number = {10},
	journaltitle = {International Journal of Communications, Network and System Sciences},
	shortjournal = {{IJCNS}},
	author = {Teng, Xiaoyun and Zhang, Xiaoyi and Yu, Hongyi},
	urldate = {2024-07-07},
	date = {2010},
	file = {Full Text:C\:\\Users\\linds\\Zotero\\storage\\2SQHX34E\\Teng et al. - 2010 - Using Least Squares Support Vector Machines for Fr.pdf:application/pdf},
}

@online{noauthor_methods_nodate,
	title = {Some methods for classification and analysis of multivariate observations},
	url = {https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fifth-Berkeley-Symposium-on-Mathematical-Statistics-and/chapter/Some-methods-for-classification-and-analysis-of-multivariate-observations/bsmsp/1200512992},
	urldate = {2024-07-07},
	file = {Some methods for classification and analysis of multivariate observations:C\:\\Users\\linds\\Zotero\\storage\\AFAGPXLF\\1200512992.html:text/html},
}

@online{noauthor_data_nodate,
	title = {Data clustering: 50 years beyond K-means - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/abs/pii/S0167865509002323},
	urldate = {2024-07-07},
	file = {Data clustering\: 50 years beyond K-means - ScienceDirect:C\:\\Users\\linds\\Zotero\\storage\\IQ5BT7QX\\S0167865509002323.html:text/html},
}

@inproceedings{elkan_using_2003,
	location = {Washington, {DC}, {USA}},
	title = {Using the triangle inequality to accelerate k-means},
	isbn = {978-1-57735-189-4},
	series = {{ICML}'03},
	abstract = {The k-means algorithm is by far the most widely used method for discovering clusters in data. We show how to accelerate it dramatically, while still always computing exactly the same result as the standard algorithm. The accelerated algorithm avoids unnecessary distance calculations by applying the triangle inequality in two different ways, and by keeping track of lower and upper bounds for distances between points and centers. Experiments show that the new algorithm is effective for datasets with up to 1000 dimensions, and becomes more and more effective as the number k of clusters increases. For k ≥ 20 it is many times faster than the best previously known accelerated k-means method.},
	pages = {147--153},
	booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
	publisher = {{AAAI} Press},
	author = {Elkan, Charles},
	urldate = {2024-07-07},
	date = {2003-08-21},
}

@article{hartigan_algorithm_1979,
	title = {Algorithm {AS} 136: A K-Means Clustering Algorithm},
	volume = {28},
	issn = {0035-9254},
	url = {https://www.jstor.org/stable/2346830},
	doi = {10.2307/2346830},
	shorttitle = {Algorithm {AS} 136},
	pages = {100--108},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Hartigan, J. A. and Wong, M. A.},
	urldate = {2024-07-07},
	date = {1979},
	note = {Publisher: [Royal Statistical Society, Oxford University Press]},
}

@online{noauthor_survey_nodate,
	title = {Survey of clustering algorithms {\textbar} {IEEE} Journals \& Magazine {\textbar} {IEEE} Xplore},
	url = {https://ieeexplore.ieee.org/document/1427769},
	urldate = {2024-07-07},
	file = {Survey of clustering algorithms | IEEE Journals & Magazine | IEEE Xplore:C\:\\Users\\linds\\Zotero\\storage\\HDARMUEU\\1427769.html:text/html},
}

@online{noauthor_survey_nodate-1,
	title = {A Survey of Clustering Data Mining Techniques {\textbar} {SpringerLink}},
	url = {https://link.springer.com/chapter/10.1007/3-540-28349-8_2},
	urldate = {2024-07-07},
	file = {A Survey of Clustering Data Mining Techniques | SpringerLink:C\:\\Users\\linds\\Zotero\\storage\\4LABMVXE\\3-540-28349-8_2.html:text/html},
}

@inproceedings{arthur_k-means_2007,
	title = {K-Means++: The Advantages of Careful Seeding},
	volume = {8},
	doi = {10.1145/1283383.1283494},
	shorttitle = {K-Means++},
	abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
	eventtitle = {Proc. of the Annu. {ACM}-{SIAM} Symp. on Discrete Algorithms},
	pages = {1027--1035},
	author = {Arthur, David and Vassilvitskii, Sergei},
	date = {2007-01-01},
}

@article{liu_survey_2017,
	title = {A survey of deep neural network architectures and their applications},
	volume = {234},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231216315533},
	doi = {10.1016/j.neucom.2016.12.038},
	abstract = {Since the proposal of a fast learning algorithm for deep belief networks in 2006, the deep learning techniques have drawn ever-increasing research interests because of their inherent capability of overcoming the drawback of traditional algorithms dependent on hand-designed features. Deep learning approaches have also been found to be suitable for big data analysis with successful applications to computer vision, pattern recognition, speech recognition, natural language processing, and recommendation systems. In this paper, we discuss some widely-used deep learning architectures and their practical applications. An up-to-date overview is provided on four deep learning architectures, namely, autoencoder, convolutional neural network, deep belief network, and restricted Boltzmann machine. Different types of deep neural networks are surveyed and recent progresses are summarized. Applications of deep learning techniques on some selected areas (speech recognition, pattern recognition and computer vision) are highlighted. A list of future research topics are finally given with clear justifications.},
	pages = {11--26},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Liu, Weibo and Wang, Zidong and Liu, Xiaohui and Zeng, Nianyin and Liu, Yurong and Alsaadi, Fuad E.},
	urldate = {2024-07-07},
	date = {2017-04-19},
	keywords = {Autoencoder, Convolutional neural network, Deep belief network, Deep learning, Restricted Boltzmann machine},
	file = {ScienceDirect Snapshot:C\:\\Users\\linds\\Zotero\\storage\\Y7KPQHW4\\S0925231216315533.html:text/html},
}

@article{ikotun_k-means_2023,
	title = {K-means clustering algorithms: A comprehensive review, variants analysis, and advances in the era of big data},
	volume = {622},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522014633},
	doi = {10.1016/j.ins.2022.11.139},
	shorttitle = {K-means clustering algorithms},
	abstract = {Advances in recent techniques for scientific data collection in the era of big data allow for the systematic accumulation of large quantities of data at various data-capturing sites. Similarly, exponential growth in the development of different data analysis approaches has been reported in the literature, amongst which the K-means algorithm remains the most popular and straightforward clustering algorithm. The broad applicability of the algorithm in many clustering application areas can be attributed to its implementation simplicity and low computational complexity. However, the K-means algorithm has many challenges that negatively affect its clustering performance. In the algorithm’s initialization process, users must specify the number of clusters in a given dataset apriori while the initial cluster centers are randomly selected. Furthermore, the algorithm's performance is susceptible to the selection of this initial cluster and for large datasets, determining the optimal number of clusters to start with becomes complex and is a very challenging task. Moreover, the random selection of the initial cluster centers sometimes results in minimal local convergence due to its greedy nature. A further limitation is that certain data object features are used in determining their similarity by using the Euclidean distance metric as a similarity measure, but this limits the algorithm’s robustness in detecting other cluster shapes and poses a great challenge in detecting overlapping clusters. Many research efforts have been conducted and reported in literature with regard to improving the K-means algorithm’s performance and robustness. The current work presents an overview and taxonomy of the K-means clustering algorithm and its variants. The history of the K-means, current trends, open issues and challenges, and recommended future research perspectives are also discussed.},
	pages = {178--210},
	journaltitle = {Information Sciences},
	shortjournal = {Information Sciences},
	author = {Ikotun, Abiodun M. and Ezugwu, Absalom E. and Abualigah, Laith and Abuhaija, Belal and Heming, Jia},
	urldate = {2024-07-07},
	date = {2023-04-01},
	keywords = {Big data clustering, Clustering algorithm, Improved k-means, K-means, K-means variants, Modified k-means, Perspectives on big data clustering},
	file = {ScienceDirect Snapshot:C\:\\Users\\linds\\Zotero\\storage\\IJRLZ4M5\\S0020025522014633.html:text/html},
}

@online{noauthor_outlier_nodate,
	title = {Outlier detection: How to Select k for k-nearest-neighbors-based outlier detectors - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/abs/pii/S0167865523002404},
	urldate = {2024-07-07},
	file = {Outlier detection\: How to Select k for k-nearest-neighbors-based outlier detectors - ScienceDirect:C\:\\Users\\linds\\Zotero\\storage\\EPHBGA8D\\S0167865523002404.html:text/html},
}

@online{noauthor_review_nodate,
	title = {Review on determining number of Cluster in K-Means Clustering {\textbar} Semantic Scholar},
	url = {https://www.semanticscholar.org/paper/Review-on-determining-number-of-Cluster-in-K-Means-Kodinariya-Makwana/1a34936bffe558a380168b790dc37956813514ba},
	urldate = {2024-07-07},
	file = {Review on determining number of Cluster in K-Means Clustering | Semantic Scholar:C\:\\Users\\linds\\Zotero\\storage\\485BKN5J\\1a34936bffe558a380168b790dc37956813514ba.html:text/html},
}

@book{pang-ning_tan_introduction_2006,
	title = {Introduction to Data Mining},
	isbn = {978-0-321-42052-7},
	publisher = {Pearson Education},
	author = {{Pang-Ning Tan} and Steinbach, Michael and {Vipin Kumar}},
	date = {2006},
}

@book{hastie_elements_2009,
	edition = {2},
	title = {The Elements of Statistical learning, Second Edition : Data mining, inference, and Prediction},
	isbn = {978-0-387-84857-0},
	url = {https://hastie.su.domains/Papers/ESLII.pdf},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	date = {2009},
}

@article{fisher_use_1936,
	title = {The Use of Multiple Measurements in Taxonomic Problems},
	volume = {7},
	rights = {1936 Blackwell Publishing Ltd/University College London},
	issn = {2050-1439},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
	doi = {10.1111/j.1469-1809.1936.tb02137.x},
	abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
	pages = {179--188},
	number = {2},
	journaltitle = {Annals of Eugenics},
	author = {Fisher, R. A.},
	urldate = {2024-07-08},
	date = {1936},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x},
	file = {Full Text PDF:C\:\\Users\\linds\\Zotero\\storage\\48BUANFA\\Fisher - 1936 - The Use of Multiple Measurements in Taxonomic Prob.pdf:application/pdf;Snapshot:C\:\\Users\\linds\\Zotero\\storage\\6QUIIRK3\\j.1469-1809.1936.tb02137.html:text/html},
}

@article{ikotun_k-means_2023-1,
	title = {K-means clustering algorithms: A comprehensive review, variants analysis, and advances in the era of big data},
	volume = {622},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522014633},
	doi = {10.1016/j.ins.2022.11.139},
	shorttitle = {K-means clustering algorithms},
	abstract = {Advances in recent techniques for scientific data collection in the era of big data allow for the systematic accumulation of large quantities of data at various data-capturing sites. Similarly, exponential growth in the development of different data analysis approaches has been reported in the literature, amongst which the K-means algorithm remains the most popular and straightforward clustering algorithm. The broad applicability of the algorithm in many clustering application areas can be attributed to its implementation simplicity and low computational complexity. However, the K-means algorithm has many challenges that negatively affect its clustering performance. In the algorithm’s initialization process, users must specify the number of clusters in a given dataset apriori while the initial cluster centers are randomly selected. Furthermore, the algorithm's performance is susceptible to the selection of this initial cluster and for large datasets, determining the optimal number of clusters to start with becomes complex and is a very challenging task. Moreover, the random selection of the initial cluster centers sometimes results in minimal local convergence due to its greedy nature. A further limitation is that certain data object features are used in determining their similarity by using the Euclidean distance metric as a similarity measure, but this limits the algorithm’s robustness in detecting other cluster shapes and poses a great challenge in detecting overlapping clusters. Many research efforts have been conducted and reported in literature with regard to improving the K-means algorithm’s performance and robustness. The current work presents an overview and taxonomy of the K-means clustering algorithm and its variants. The history of the K-means, current trends, open issues and challenges, and recommended future research perspectives are also discussed.},
	pages = {178--210},
	journaltitle = {Information Sciences},
	shortjournal = {Information Sciences},
	author = {Ikotun, Abiodun M. and Ezugwu, Absalom E. and Abualigah, Laith and Abuhaija, Belal and Heming, Jia},
	urldate = {2024-07-08},
	date = {2023-04-01},
	keywords = {Big data clustering, Clustering algorithm, Improved k-means, K-means, K-means variants, Modified k-means, Perspectives on big data clustering},
	file = {ScienceDirect Snapshot:C\:\\Users\\linds\\Zotero\\storage\\FK4QGJI9\\S0020025522014633.html:text/html},
}

@article{lund_review_2021,
	title = {A review of cluster analysis techniques and their uses in library and information science research: k-means and k-medoids clustering},
	volume = {22},
	issn = {1467-8047},
	url = {https://doi.org/10.1108/PMM-05-2021-0026},
	doi = {10.1108/PMM-05-2021-0026},
	shorttitle = {A review of cluster analysis techniques and their uses in library and information science research},
	abstract = {Purpose This literature review explores the definitions and characteristics of cluster analysis, a machine-learning technique that is frequently implemented to identify groupings in big datasets and its applicability to library and information science ({LIS}) research. This overview is intended for researchers who are interested in expanding their data analysis repertory to include cluster analysis, rather than for existing experts in this area. Design/methodology/approach A review of {LIS} articles included in the Library and Information Source ({EBSCO}) database that employ cluster analysis is performed. An overview of cluster analysis in general (how it works from a statistical standpoint, and how it can be performed by researchers), the most popular cluster analysis techniques and the uses of cluster analysis in {LIS} is presented. Findings The number of {LIS} studies that employ a cluster analytic approach has grown from about 5 per year in the early 2000s to an average of 35 studies per year in the mid- and late-2010s. The journal Scientometrics has the most articles published within {LIS} that use cluster analysis (102 studies). Scientometrics is the most common subject area to employ a cluster analytic approach (152 studies). The findings of this review indicate that cluster analysis could make {LIS} research more accessible by providing an innovative and insightful process of knowledge discovery. Originality/value This review is the first to present cluster analysis as an accessible data analysis approach, specifically from an {LIS} perspective.},
	pages = {161--173},
	number = {3},
	journaltitle = {Performance Measurement and Metrics},
	author = {Lund, Brady and Ma, Jinxuan},
	urldate = {2024-07-08},
	date = {2021-01-01},
	note = {Publisher: Emerald Publishing Limited},
	keywords = {Cluster analysis, Clustering, Data analysis, K-means, Library and information science, Research methods},
}

@article{brusco_comparison_2017,
	title = {A comparison of latent class, K-means, and K-median methods for clustering dichotomous data},
	volume = {22},
	issn = {1939-1463},
	doi = {10.1037/met0000095},
	abstract = {The problem of partitioning a collection of objects based on their measurements on a set of dichotomous variables is a well-established problem in psychological research, with applications including clinical diagnosis, educational testing, cognitive categorization, and choice analysis. Latent class analysis and K-means clustering are popular methods for partitioning objects based on dichotomous measures in the psychological literature. The K-median clustering method has recently been touted as a potentially useful tool for psychological data and might be preferable to its close neighbor, K-means, when the variable measures are dichotomous. We conducted simulation-based comparisons of the latent class, K-means, and K-median approaches for partitioning dichotomous data. Although all 3 methods proved capable of recovering cluster structure, K-median clustering yielded the best average performance, followed closely by latent class analysis. We also report results for the 3 methods within the context of an application to transitive reasoning data, in which it was found that the 3 approaches can exhibit profound differences when applied to real data. ({PsycINFO} Database Record (c) 2019 {APA}, all rights reserved)},
	pages = {563--580},
	number = {3},
	journaltitle = {Psychological Methods},
	author = {Brusco, Michael J. and Shireman, Emilie and Steinley, Douglas},
	date = {2017},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Choice Behavior, Classification (Cognitive Process), Cluster Analysis, Diagnosis, Latent Class Analysis, Mathematical Modeling, Mean, Median, Methodology, Statistical Data, Testing},
	file = {Accepted Version:C\:\\Users\\linds\\Zotero\\storage\\X7R3LZWW\\Brusco et al. - 2017 - A comparison of latent class, K-means, and K-media.pdf:application/pdf;Snapshot:C\:\\Users\\linds\\Zotero\\storage\\PWT5PX3R\\doiLanding.html:text/html},
}

@article{tleis_segmenting_2017,
	title = {Segmenting the organic food market in Lebanon: an application of k-means cluster analysis},
	volume = {119},
	issn = {0007-070X},
	url = {https://doi.org/10.1108/BFJ-08-2016-0354},
	doi = {10.1108/BFJ-08-2016-0354},
	shorttitle = {Segmenting the organic food market in Lebanon},
	abstract = {Purpose The purpose of this paper is to discover profiles of organic food consumers in Lebanon by performing a market segmentation based on lifestyle and attitude variables and thus be able to propose appropriate marketing strategies for each market segment. Design/methodology/approach A survey, based on the use of closed-ended questionnaire, was addressed to 320 organic food consumers in the capital Beirut, in February and March 2014. Descriptive analysis, principal component analysis and cluster analysis (k-means method) were performed upon collected data. Findings Four clusters were obtained and labelled based on psychographic characteristics and willingness to pay for the most purchased organic products. “Localist” and “Health conscious” clusters were the largest proportion of the selected sample, thus these were the most critical to be addressed by specific marketing strategies, emphasising the combination of local and organic food and the healthy properties of organic products. “Rational” and “Irregular” cluster were relatively small groups, addressed by pricing and promotional strategies. Originality/value This is the first study attempting to segment organic food consumers into different categories in a developing country as Lebanon.},
	pages = {1423--1441},
	number = {7},
	journaltitle = {British Food Journal},
	author = {Tleis, Malak and Callieris, Roberta and Roma, Rocco},
	urldate = {2024-07-09},
	date = {2017-01-01},
	note = {Publisher: Emerald Publishing Limited},
	keywords = {Consumer analysis and profiling, Lebanese organic market, Organic market segmentation, {PCA} and cluster analysis},
	file = {Snapshot:C\:\\Users\\linds\\Zotero\\storage\\J4IS8PQ2\\html.html:text/html},
}

@online{noauthor_sustainability_nodate,
	title = {Sustainability {\textbar} Free Full-Text {\textbar} Profiling Citizens on Perception of Key Factors of Food Security: An Application of K-Means Cluster Analysis},
	url = {https://www.mdpi.com/2071-1050/15/13/9915},
	urldate = {2024-07-09},
	file = {Sustainability | Free Full-Text | Profiling Citizens on Perception of Key Factors of Food Security\: An Application of K-Means Cluster Analysis:C\:\\Users\\linds\\Zotero\\storage\\T8GIFZ3U\\9915.html:text/html},
}

@article{facendola_profiling_2023,
	title = {Profiling Citizens on Perception of Key Factors of Food Security: An Application of K-Means Cluster Analysis},
	volume = {15},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2071-1050},
	url = {https://www.mdpi.com/2071-1050/15/13/9915},
	doi = {10.3390/su15139915},
	shorttitle = {Profiling Citizens on Perception of Key Factors of Food Security},
	abstract = {Cities have been increasingly involved in the development of food policies, becoming key points in achieving food security and fostering the transition to sustainable agri-food systems. The aim of this paper is to identify citizens’ profiles by performing segmentation and profiling according to their socio-economic variables and perception of key factors affecting food security. This is to define appropriate strategies to guide policy makers in a more effective creation of urban food policies. An online survey was filled out by citizens of the Metropolitan City of Bari from July to November 2022. Descriptive analysis, principal component analysis and K-means cluster analysis were applied to the collected data. Four clusters of citizens were obtained and labelled based on socio-economic characteristics and key factors affecting food security perception. Specifically, the “Law-confident” (45\% of citizens) and “Hedonist” (36\%) clusters revealed the greatest trust in “governance” and “quality certification” aspects. The “Capitalist” (15\%) and “Conservatory” (4\%) clusters were relatively small groups, characterized respectively by a positive perception of the standardization of food production and governance power, with a focus on strategies regarding food policy implementation, reduction of food loss and waste ({FLW}) and improvement of food quality certifications systems. The proposed approach and results may support {EU} policy makers in identifying key macro-areas and matters toward which to direct public funding in order to improve food security in urban areas, and to put in place actions enhancing citizens’ knowledge and awareness of key issues of food security.},
	pages = {9915},
	number = {13},
	journaltitle = {Sustainability},
	author = {Facendola, Rosalia and Ottomano Palmisano, Giovanni and De Boni, Annalisa and Acciani, Claudio and Roma, Rocco},
	urldate = {2024-07-09},
	date = {2023-01},
	langid = {english},
	note = {Number: 13
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {agri-food systems, cluster analysis, food security, {PCA}, sustainable food systems, urban food policy, urban planning},
	file = {Full Text PDF:C\:\\Users\\linds\\Zotero\\storage\\P7VBHZ9B\\Facendola et al. - 2023 - Profiling Citizens on Perception of Key Factors of.pdf:application/pdf},
}

@article{yang_outlier_2023,
	title = {Outlier detection: How to Select k{\textless}math{\textgreater}{\textless}mi is="true"{\textgreater}k{\textless}/mi{\textgreater}{\textless}/math{\textgreater} for k{\textless}math{\textgreater}{\textless}mi is="true"{\textgreater}k{\textless}/mi{\textgreater}{\textless}/math{\textgreater}-nearest-neighbors-based outlier detectors},
	volume = {174},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865523002404},
	doi = {10.1016/j.patrec.2023.08.020},
	shorttitle = {Outlier detection},
	abstract = {Unsupervised k-nearest-neighbor-based outlier detectors play a vital role in data science research. However, the detectors’ performance relies on the choice of the parameter k. However, autonomous selection of the optimal k is poorly documented in literature as it is very challenging. Conventional methods prove ineffective and lack universality as they fail to account for both application and detector factors simultaneously. This article proposes neighborhood consistency, a new concept which tackles the existing issues of selecting the optimal k by considering both application and detector factors. This concept was used to develop a method termed k finder based on neighborhood consistency ({KFC}). {KFC} does not rely on any extra parameter and has linear time complexity. Simulations show that {KFC} outperformed baselines and had a good generality to different datasets and detectors. The implementation of the proposed methods can be found on www.{OutlierNet}.com for reproducibility.},
	pages = {112--117},
	journaltitle = {Pattern Recognition Letters},
	shortjournal = {Pattern Recognition Letters},
	author = {Yang, Jiawei and Tan, Xu and Rahardja, Sylwan},
	urldate = {2024-07-09},
	date = {2023-10-01},
	keywords = {-nearest neighbors, -{NN}, {KFC}, neighborhood consistency, Neighborhood-based outlier detectors, Outlier detection},
	file = {ScienceDirect Snapshot:C\:\\Users\\linds\\Zotero\\storage\\WH2T3MV8\\S0167865523002404.html:text/html},
}

@online{noauthor_review_nodate-1,
	title = {Review on determining number of Cluster in K-Means Clustering {\textbar} Semantic Scholar},
	url = {https://www.semanticscholar.org/paper/Review-on-determining-number-of-Cluster-in-K-Means-Kodinariya-Makwana/1a34936bffe558a380168b790dc37956813514ba},
	urldate = {2024-07-09},
}

@inproceedings{kodinariya_review_2013,
	title = {Review on determining number of Cluster in K-Means Clustering},
	url = {https://www.semanticscholar.org/paper/Review-on-determining-number-of-Cluster-in-K-Means-Kodinariya-Makwana/1a34936bffe558a380168b790dc37956813514ba},
	abstract = {Clustering is widely used in different field such as biology, psychology, and economics. The result of clustering varies as number of cluster parameter changes hence main challenge of cluster analysis is that the number of clusters or the number of model parameters is seldom known, and it must be determined before clustering. The several clustering algorithm has been proposed. Among them k-means method is a simple and fast clustering technique. We address the problem of cluster number selection by using a k-means approach We can ask end users to provide a number of clusters in advance, but it is not feasible end user requires domain knowledge of each data set. There are many methods available to estimate the number of clusters such as statistical indices, variance based method, Information Theoretic, goodness of fit method etc...The paper explores six different approaches to determine the right number of clusters in a dataset},
	author = {Kodinariya, Trupti M. and Makwana, Prashant R.},
	urldate = {2024-07-09},
	date = {2013},
}

@incollection{berkhin_survey_2006,
	location = {Berlin, Heidelberg},
	title = {A Survey of Clustering Data Mining Techniques},
	isbn = {978-3-540-28349-2},
	url = {https://doi.org/10.1007/3-540-28349-8_2},
	abstract = {Clustering is the division of data into groups of similar objects. In clustering, some details are disregarded in exchange for data simplification. Clustering can be viewed as a data modeling technique that provides for concise summaries of the data. Clustering is therefore related to many disciplines and plays an important role in a broad range of applications. The applications of clustering usually deal with large datasets and data with many attributes. Exploration of such data is a subject of data mining. This survey concentrates on clustering algorithms from a data mining perspective.},
	pages = {25--71},
	booktitle = {Grouping Multidimensional Data: Recent Advances in Clustering},
	publisher = {Springer Berlin Heidelberg},
	author = {Berkhin, P.},
	editor = {Kogan, Jacob and Nicholas, Charles and Teboulle, Marc},
	date = {2006},
	doi = {10.1007/3-540-28349-8_2},
}

@article{gan_k-means_2017,
	title = {\textit{k}-means clustering with outlier removal},
	volume = {90},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865517300740},
	doi = {10.1016/j.patrec.2017.03.008},
	abstract = {Outlier detection is an important data analysis task in its own right and removing the outliers from clusters can improve the clustering accuracy. In this paper, we extend the k-means algorithm to provide data clustering and outlier detection simultaneously by introducing an additional “cluster” to the k-means algorithm to hold all outliers. We design an iterative procedure to optimize the objective function of the proposed algorithm and establish the convergence of the iterative procedure. Numerical experiments on both synthetic data and real data are provided to demonstrate the effectiveness and efficiency of the proposed algorithm.},
	pages = {8--14},
	journaltitle = {Pattern Recognition Letters},
	shortjournal = {Pattern Recognition Letters},
	author = {Gan, Guojun and Ng, Michael Kwok-Po},
	urldate = {2024-07-09},
	date = {2017-04-15},
	keywords = {-means, Data clustering, Outlier detection},
	file = {ScienceDirect Snapshot:C\:\\Users\\linds\\Zotero\\storage\\5F3IHIUQ\\S0167865517300740.html:text/html},
}

@article{zhao_k-means_2018,
	title = {\textit{k}-means: A revisit},
	volume = {291},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S092523121830239X},
	doi = {10.1016/j.neucom.2018.02.072},
	shorttitle = {\textit{k}-means},
	abstract = {Due to its simplicity and versatility, k-means remains popular since it was proposed three decades ago. The performance of k-means has been enhanced from different perspectives over the years. Unfortunately, a good trade-off between quality and efficiency is hardly reached. In this paper, a novel k-means variant is presented. Different from most of k-means variants, the clustering procedure is driven by an explicit objective function, which is feasible for the whole l2-space. The classic egg-chicken loop in k-means has been simplified to a pure stochastic optimization procedure. The procedure of k-means becomes simpler and converges to a considerably better local optima. The effectiveness of this new variant has been studied extensively in different contexts, such as document clustering, nearest neighbor search and image clustering. Superior performance is observed across different scenarios.},
	pages = {195--206},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Zhao, Wan-Lei and Deng, Cheng-Hao and Ngo, Chong-Wah},
	urldate = {2024-07-09},
	date = {2018-05-24},
	keywords = {-means, Clustering, Incremental optimization},
	file = {ScienceDirect Snapshot:C\:\\Users\\linds\\Zotero\\storage\\42B7XAEV\\S092523121830239X.html:text/html},
}

@article{ikotun_k-means_2023-2,
	title = {K-means clustering algorithms: A comprehensive review, variants analysis, and advances in the era of big data},
	volume = {622},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522014633},
	doi = {10.1016/j.ins.2022.11.139},
	shorttitle = {K-means clustering algorithms},
	abstract = {Advances in recent techniques for scientific data collection in the era of big data allow for the systematic accumulation of large quantities of data at various data-capturing sites. Similarly, exponential growth in the development of different data analysis approaches has been reported in the literature, amongst which the K-means algorithm remains the most popular and straightforward clustering algorithm. The broad applicability of the algorithm in many clustering application areas can be attributed to its implementation simplicity and low computational complexity. However, the K-means algorithm has many challenges that negatively affect its clustering performance. In the algorithm’s initialization process, users must specify the number of clusters in a given dataset apriori while the initial cluster centers are randomly selected. Furthermore, the algorithm's performance is susceptible to the selection of this initial cluster and for large datasets, determining the optimal number of clusters to start with becomes complex and is a very challenging task. Moreover, the random selection of the initial cluster centers sometimes results in minimal local convergence due to its greedy nature. A further limitation is that certain data object features are used in determining their similarity by using the Euclidean distance metric as a similarity measure, but this limits the algorithm’s robustness in detecting other cluster shapes and poses a great challenge in detecting overlapping clusters. Many research efforts have been conducted and reported in literature with regard to improving the K-means algorithm’s performance and robustness. The current work presents an overview and taxonomy of the K-means clustering algorithm and its variants. The history of the K-means, current trends, open issues and challenges, and recommended future research perspectives are also discussed.},
	pages = {178--210},
	journaltitle = {Information Sciences},
	shortjournal = {Information Sciences},
	author = {Ikotun, Abiodun M. and Ezugwu, Absalom E. and Abualigah, Laith and Abuhaija, Belal and Heming, Jia},
	urldate = {2024-07-09},
	date = {2023-04-01},
	keywords = {Big data clustering, Clustering algorithm, Improved k-means, K-means, K-means variants, Modified k-means, Perspectives on big data clustering},
	file = {ScienceDirect Snapshot:C\:\\Users\\linds\\Zotero\\storage\\AYQ44N3G\\S0020025522014633.html:text/html},
}

@article{ikotun_k-means_2023-3,
	title = {K-means clustering algorithms: A comprehensive review, variants analysis, and advances in the era of big data},
	volume = {622},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025522014633},
	doi = {10.1016/j.ins.2022.11.139},
	shorttitle = {K-means clustering algorithms},
	abstract = {Advances in recent techniques for scientific data collection in the era of big data allow for the systematic accumulation of large quantities of data at various data-capturing sites. Similarly, exponential growth in the development of different data analysis approaches has been reported in the literature, amongst which the K-means algorithm remains the most popular and straightforward clustering algorithm. The broad applicability of the algorithm in many clustering application areas can be attributed to its implementation simplicity and low computational complexity. However, the K-means algorithm has many challenges that negatively affect its clustering performance. In the algorithm’s initialization process, users must specify the number of clusters in a given dataset apriori while the initial cluster centers are randomly selected. Furthermore, the algorithm's performance is susceptible to the selection of this initial cluster and for large datasets, determining the optimal number of clusters to start with becomes complex and is a very challenging task. Moreover, the random selection of the initial cluster centers sometimes results in minimal local convergence due to its greedy nature. A further limitation is that certain data object features are used in determining their similarity by using the Euclidean distance metric as a similarity measure, but this limits the algorithm’s robustness in detecting other cluster shapes and poses a great challenge in detecting overlapping clusters. Many research efforts have been conducted and reported in literature with regard to improving the K-means algorithm’s performance and robustness. The current work presents an overview and taxonomy of the K-means clustering algorithm and its variants. The history of the K-means, current trends, open issues and challenges, and recommended future research perspectives are also discussed.},
	pages = {178--210},
	journaltitle = {Information Sciences},
	shortjournal = {Information Sciences},
	author = {Ikotun, Abiodun M. and Ezugwu, Absalom E. and Abualigah, Laith and Abuhaija, Belal and Heming, Jia},
	urldate = {2024-07-09},
	date = {2023-04-01},
	keywords = {Big data clustering, Clustering algorithm, Improved k-means, K-means, K-means variants, Modified k-means, Perspectives on big data clustering},
	file = {ScienceDirect Snapshot:C\:\\Users\\linds\\Zotero\\storage\\Q8EDI8XJ\\S0020025522014633.html:text/html},
}
@article{LI20121104,
title = {A Clustering Method Based on K-Means Algorithm},
journal = {Physics Procedia},
volume = {25},
pages = {1104-1109},
year = {2012},
note = {International Conference on Solid State Devices and Materials Science, April 1-2, 2012, Macao},
issn = {1875-3892},
doi = {https://doi.org/10.1016/j.phpro.2012.03.206},
url = {https://www.sciencedirect.com/science/article/pii/S1875389212006220},
author = {Youguo Li and Haiyan Wu},
keywords = {cluster analysis, K-Means algorithm, distance algorithm, samples of pattern},
abstract = {In this paper we combine the largest minimum distance algorithm and the traditional K-Means algorithm to propose an improved K-Means clustering algorithm. This improved algorithm can make up the shortcomings for the traditional K-Means algorithm to determine the initial focal point. The improved K-Means algorithm effectively solved two disadvantages of the traditional algorithm, the first one is greater dependence to choice the initial focal point, and another one is easy to be trapped in local minimum[1], [2].}
}


@article{higuera_self-organizing_2015,
	title = {Self-{Organizing} {Feature} {Maps} {Identify} {Proteins} {Critical} to {Learning} in a {Mouse} {Model} of {Down} {Syndrome}},
	volume = {10},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0129126},
	doi = {10.1371/journal.pone.0129126},
	abstract = {Down syndrome (DS) is a chromosomal abnormality (trisomy of human chromosome 21) associated with intellectual disability and affecting approximately one in 1000 live births worldwide. The overexpression of genes encoded by the extra copy of a normal chromosome in DS is believed to be sufficient to perturb normal pathways and normal responses to stimulation, causing learning and memory deficits. In this work, we have designed a strategy based on the unsupervised clustering method, Self Organizing Maps (SOM), to identify biologically important differences in protein levels in mice exposed to context fear conditioning (CFC). We analyzed expression levels of 77 proteins obtained from normal genotype control mice and from their trisomic littermates (Ts65Dn) both with and without treatment with the drug memantine. Control mice learn successfully while the trisomic mice fail, unless they are first treated with the drug, which rescues their learning ability. The SOM approach identified reduced subsets of proteins predicted to make the most critical contributions to normal learning, to failed learning and rescued learning, and provides a visual representation of the data that allows the user to extract patterns that may underlie novel biological responses to the different kinds of learning and the response to memantine. Results suggest that the application of SOM to new experimental data sets of complex protein profiles can be used to identify common critical protein responses, which in turn may aid in identifying potentially more effective drug targets.},
	language = {en},
	number = {6},
	urldate = {2024-07-23},
	journal = {PLOS ONE},
	author = {Higuera, Clara and Gardiner, Katheleen J. and Cios, Krzysztof J.},
	month = jun,
	year = {2015},
	note = {Publisher: Public Library of Science},
	keywords = {Down syndrome, Drug therapy, Learning, MAPK signaling cascades, Mouse models, Protein expression, Receptor antagonist therapy, Trisomics},
	pages = {e0129126},
	file = {Full Text PDF:C\:\\Users\\linds\\Zotero\\storage\\3MFEKFS9\\Higuera et al. - 2015 - Self-Organizing Feature Maps Identify Proteins Cri.pdf:application/pdf},
}

